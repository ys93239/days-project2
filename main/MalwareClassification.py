from __future__ import print_function
import urllib

from pyspark.sql import SQLContext, Row,SparkSession
from pyspark.ml.feature import IDF, Tokenizer,HashingTF
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.types import *
from pyspark.ml.feature import CountVectorizer
from pyspark.sql.functions import lit
import sys
from pyspark.ml.feature import NGram


import re
import os


spark = SparkSession\
        .builder\
        .appName("MalwareClassification")\
        .getOrCreate()
sc=spark.sparkContext;
sqlContext=SQLContext(sc)


########Main method for Main
def main(args):
    fields = [StructField("hashcodefile", StringType(), True), StructField("label", StringType(), True),
              StructField("n-grams", ArrayType(StringType(), True), True)]

    schema = StructType(fields)
    fieldsTest = [StructField("hashcodefile", StringType(), True),
              StructField("n-grams", ArrayType(StringType(), True), True)]
    schemaTest = StructType(fieldsTest)
    ## args[0] Preprocessd  training Parquet file of byte or opcode
    trainingParque=spark.read.parquet(args[0])
    print ("Parquet File read completed")

    #Creating for Trainig and Testing N-Gram
    # args[1]: No of grams: 1,2,3,4,....N
    ngram = NGram(n=args[1], inputCol="content", outputCol="n-grams")
    ngramDataFrame = ngram.transform(trainingParque).select("hashcodefile","label","n-grams")
    ngramRDD = ngramDataFrame.rdd

    # args[2]: Preprocessed Testing Parquet file of byte or opcode
    testingParqueTemp = spark.read.parquet(args[2])
    ngramTestData = NGram(n=args[1], inputCol="content", outputCol="n-grams")
    ngramTestDataFrame = ngramTestData.transform(testingParqueTemp).select("hashcodefile","n-grams")
    ngramTestDataRDD=ngramTestDataFrame.rdd
    inputNgram=spark.createDataFrame(ngramRDD,schema)
    inputTestNgram = spark.createDataFrame(ngramTestDataRDD, schemaTest)

    print("N-gram completed for testing & training")

    ################################################################################
    # Count Vectorizer for training data set
    ################################################################################

    cv = CountVectorizer(inputCol="n-grams", outputCol="features", vocabSize=1000, minDF=1.0,minTF=2.0)
    model = cv.fit(inputNgram)
    featurizedData = model.transform(inputNgram).select("hashcodefile","label","features")
    print ("Term Frequency completed for training data set")

    # # ######################################
    # Count Vectorizer for testing data set
    # # ######################################
    cvTest = CountVectorizer(inputCol="n-grams", outputCol="features", vocabSize=1000, minDF=1.0,minTF=2.0)

    modelTest = cvTest.fit(inputTestNgram)

    featurizedTestData = modelTest.transform(inputTestNgram).select("hashcodefile","features")
    featurizedTestData.write.parquet("opcodeFeaturesTesting.parquet")

    print("Term Frequency completed for testing data set")

    ###################################################################
    # Code for Random Forest Classifier
    ##################################################################
    labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(featurizedData)
    # # Train a RandomForest model.
    randomforest = RandomForestClassifier(labelCol="indexedLabel", featuresCol="features", numTrees=600,maxDepth=10)

    #  Convert indexed labels back to original labels.
    labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel",
                                    labels=labelIndexer.labels)
    # # Chain indexers and forest in a Pipeline
    pipeline = Pipeline(stages=[labelIndexer,randomforest,labelConverter])

    # # Train model.  This also runs the indexers.
    model = pipeline.fit(featurizedData)
    predictions = model.transform(featurizedTestData)
    filterPredictions=predictions.select("predictedLabel","hashcodefile")
    predictionsRDD=filterPredictions.rdd

    predictionsRDD.saveAsTextFile("output.text")

    #============================================================
if __name__ == "__main__":
    main(sys.argv)